{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f8be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Transferred 272/272 items from pretrained weights\n",
      "Transferred 212/212 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from attention_approximation.modeling_llama_approximated import LlamaModel as StudentModel\n",
    "from attention_approximation.modeling_llama import LlamaForCausalLM as TeacherModel\n",
    "from transformers import LlamaConfig\n",
    "from attention_approximation.pytorch import intersect_dicts\n",
    "from attention_approximation.utils import LOGGER\n",
    "import safetensors\n",
    "import copy\n",
    "\n",
    "config = \"../data/MobileLLM/config.json\"\n",
    "w1 = \"../data/MobileLLM/model.safetensors\"\n",
    "w2 = \"/Users/gianlucacalo/Desktop/checkpoints/checkpoint_step_1001.pt\"\n",
    "\n",
    "config = LlamaConfig().from_json_file(config)\n",
    "teacher = TeacherModel(config)\n",
    "ckpt = safetensors.torch.load_file(w1)\n",
    "csd = intersect_dicts(ckpt, teacher.state_dict())\n",
    "teacher.load_state_dict(csd)\n",
    "LOGGER.info(f\"Transferred {len(csd)}/{len(teacher.state_dict())} items from pretrained weights\")\n",
    "\n",
    "\n",
    "config = copy.deepcopy(config)\n",
    "config.factorization_rank = 16\n",
    "config.layer_sharing = False\n",
    "config.seq_length = 512\n",
    "\n",
    "student = StudentModel(config)\n",
    "ckpt = torch.load(w2, map_location=\"cpu\")['model_state_dict']\n",
    "\n",
    "\n",
    "csd = intersect_dicts(ckpt, student.state_dict())\n",
    "student.load_state_dict(csd)\n",
    "LOGGER.info(f\"Transferred {len(csd)}/{len(student.state_dict())} items from pretrained weights\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-approximation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
